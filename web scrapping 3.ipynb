{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c0ffe81",
   "metadata": {},
   "source": [
    "# 1. Write a python program which searches all the product under a particular product from www.amazon.in. The\n",
    "product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for\n",
    "guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ccc3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon_product(product_name):\n",
    "    base_url = \"https://www.amazon.in/s\"\n",
    "    params = {\"k\": product_name}\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        product_containers = soup.find_all(\"div\", class_=\"s-result-item\")\n",
    "\n",
    "        for product_container in product_containers:\n",
    "            product_title = product_container.find(\"h2\").text.strip()\n",
    "            product_price = product_container.find(\"span\", class_=\"a-offscreen\")\n",
    "            if product_price:\n",
    "                product_price = product_price.text\n",
    "            else:\n",
    "                product_price = \"Price not available\"\n",
    "\n",
    "            print(\"Product:\", product_title)\n",
    "            print(\"Price:\", product_price)\n",
    "            print(\"=\" * 50)\n",
    "    else:\n",
    "        print(\"Failed to fetch Amazon page\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product to search for on Amazon.in: \")\n",
    "    search_amazon_product(user_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b2657d",
   "metadata": {},
   "source": [
    "# In the above question, now scrape the following details of each product listed in first 3 pages of your search\n",
    "results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then\n",
    "scrape all the products available under that product name. Details to be scraped are: \"Brand\n",
    "Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and\n",
    "“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb131fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def scrape_product_details(product_container):\n",
    "    product_details = {}\n",
    "\n",
    "    product_title = product_container.find(\"h2\").text.strip()\n",
    "    product_details[\"Product Name\"] = product_title\n",
    "    \n",
    "    product_brand = product_container.find(\"span\", class_=\"a-size-base-plus a-color-base a-text-normal\")\n",
    "    if product_brand:\n",
    "        product_details[\"Brand Name\"] = product_brand.text.strip()\n",
    "    else:\n",
    "        product_details[\"Brand Name\"] = \"-\"\n",
    "    \n",
    "    product_price = product_container.find(\"span\", class_=\"a-offscreen\")\n",
    "    if product_price:\n",
    "        product_details[\"Price\"] = product_price.text\n",
    "    else:\n",
    "        product_details[\"Price\"] = \"-\"\n",
    "    \n",
    "    product_return = product_container.find(\"div\", class_=\"a-column a-span3 a-text-center\").text.strip()\n",
    "    product_details[\"Return/Exchange\"] = product_return\n",
    "    \n",
    "    product_delivery = product_container.find(\"span\", class_=\"a-text-bold\").text.strip()\n",
    "    product_details[\"Expected Delivery\"] = product_delivery\n",
    "    \n",
    "    product_availability = product_container.find(\"span\", class_=\"a-declarative\").text.strip()\n",
    "    product_details[\"Availability\"] = product_availability\n",
    "    \n",
    "    product_url = product_container.find(\"a\", class_=\"a-link-normal\")\n",
    "    if product_url:\n",
    "        product_details[\"Product URL\"] = \"https://www.amazon.in\" + product_url[\"href\"]\n",
    "    else:\n",
    "        product_details[\"Product URL\"] = \"-\"\n",
    "\n",
    "    return product_details\n",
    "\n",
    "def search_amazon_product(product_name):\n",
    "    base_url = \"https://www.amazon.in/s\"\n",
    "    params = {\"k\": product_name}\n",
    "\n",
    "    product_data = []\n",
    "    page_count = 0\n",
    "\n",
    "    while len(product_data) < 3 * 16:  # Assuming 16 products per page\n",
    "        response = requests.get(base_url, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            product_containers = soup.find_all(\"div\", class_=\"s-result-item\")\n",
    "\n",
    "            for product_container in product_containers:\n",
    "                product_details = scrape_product_details(product_container)\n",
    "                product_data.append(product_details)\n",
    "\n",
    "            next_page_link = soup.find(\"li\", class_=\"a-last\")\n",
    "            if next_page_link:\n",
    "                next_page_url = \"https://www.amazon.in\" + next_page_link.find(\"a\")[\"href\"]\n",
    "                base_url = next_page_url\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            page_count += 1\n",
    "            if page_count >= 3:\n",
    "                break\n",
    "\n",
    "            time.sleep(2)  # Adding a delay to avoid overloading the server\n",
    "        else:\n",
    "            print(\"Failed to fetch Amazon page\")\n",
    "            break\n",
    "\n",
    "    return product_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product to search for on Amazon.in: \")\n",
    "    product_data = search_amazon_product(user_input)\n",
    "\n",
    "    # Creating a DataFrame from the scraped data\n",
    "    df = pd.DataFrame(product_data)\n",
    "\n",
    "    # Saving the DataFrame to a CSV file\n",
    "    csv_filename = f\"{user_input}_products.csv\"\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "\n",
    "    print(f\"Scraped data saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b8bebb",
   "metadata": {},
   "source": [
    "# Write a python program to access the search bar and search button on images.google.com and scrape 10\n",
    "images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5676ee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "# Create a new instance of the Chrome browser (you need to have Chrome and chromedriver installed)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open Google Images\n",
    "driver.get(\"https://www.google.com/imghp\")\n",
    "\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "num_images_to_scrape = 10\n",
    "\n",
    "for keyword in keywords:\n",
    "    # Find the search bar element and send the keyword\n",
    "    search_bar = driver.find_element_by_name(\"q\")\n",
    "    search_bar.clear()\n",
    "    search_bar.send_keys(keyword)\n",
    "    search_bar.send_keys(Keys.RETURN)\n",
    "    \n",
    "    # Wait for search results to load\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Scroll down to load more images (optional)\n",
    "    # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "    # Collect image elements\n",
    "    image_elements = driver.find_elements_by_css_selector(\".rg_i\")\n",
    "    \n",
    "    # Loop through image elements and download images\n",
    "    for i, img_element in enumerate(image_elements[:num_images_to_scrape]):\n",
    "        img_url = img_element.get_attribute(\"src\")\n",
    "        # You can use a library like requests to download the images\n",
    "        # Here, we'll just print the URLs\n",
    "        print(f\"{keyword} Image {i+1}: {img_url}\")\n",
    "    \n",
    "    # Wait before moving to the next keyword\n",
    "    time.sleep(2)\n",
    "\n",
    "# Close the browser window\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee260385",
   "metadata": {},
   "source": [
    "# 4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com\n",
    "and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand\n",
    "Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,\n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the\n",
    "details is missing then replace it by “- “. Save your results in a dataframe and CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c057a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "\n",
    "# Create a new instance of the Chrome browser (you need to have Chrome and chromedriver installed)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def scrape_flipkart_smartphones(product_name):\n",
    "    base_url = \"https://www.flipkart.com\"\n",
    "    search_url = f\"{base_url}/search?q={product_name}\"\n",
    "    \n",
    "    driver.get(search_url)\n",
    "\n",
    "    product_data = []\n",
    "\n",
    "    product_containers = driver.find_elements(By.CSS_SELECTOR, \"._1AtVbE\")\n",
    "\n",
    "    for product_container in product_containers:\n",
    "        details = {\n",
    "            \"Brand Name\": \"-\",\n",
    "            \"Smartphone Name\": \"-\",\n",
    "            \"Colour\": \"-\",\n",
    "            \"RAM\": \"-\",\n",
    "            \"Storage(ROM)\": \"-\",\n",
    "            \"Primary Camera\": \"-\",\n",
    "            \"Secondary Camera\": \"-\",\n",
    "            \"Display Size\": \"-\",\n",
    "            \"Battery Capacity\": \"-\",\n",
    "            \"Price\": \"-\",\n",
    "            \"Product URL\": \"-\"\n",
    "        }\n",
    "\n",
    "        product_title_element = product_container.find_element(By.CSS_SELECTOR, \"div._4rR01T\")\n",
    "        details[\"Smartphone Name\"] = product_title_element.text\n",
    "        \n",
    "        product_link_element = product_container.find_element(By.CSS_SELECTOR, \"a._1fQZEK\")\n",
    "        details[\"Product URL\"] = base_url + product_link_element.get_attribute(\"href\")\n",
    "        \n",
    "        try:\n",
    "            product_price_element = product_container.find_element(By.CSS_SELECTOR, \"div._30jeq3\")\n",
    "            details[\"Price\"] = product_price_element.text\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            rating_element = product_container.find_element(By.CSS_SELECTOR, \"div._3LWZlK\")\n",
    "            details[\"Rating\"] = rating_element.text\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        product_data.append(details)\n",
    "    \n",
    "    return product_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    product_name = input(\"Enter the smartphone name to search for on Flipkart: \")\n",
    "    product_data = scrape_flipkart_smartphones(product_name)\n",
    "\n",
    "    # Creating a DataFrame from the scraped data\n",
    "    df = pd.DataFrame(product_data)\n",
    "\n",
    "    # Saving the DataFrame to a CSV file\n",
    "    csv_filename = f\"{product_name}_smartphones.csv\"\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "\n",
    "    print(f\"Scraped data saved to {csv_filename}\")\n",
    "\n",
    "# Close the browser window\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b74301d",
   "metadata": {},
   "source": [
    "# 5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c53e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_coordinates(city_name):\n",
    "    api_key = \"YOUR_GOOGLE_MAPS_API_KEY\"\n",
    "    base_url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
    "\n",
    "    params = {\n",
    "        \"address\": city_name,\n",
    "        \"key\": api_key\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    if data[\"status\"] == \"OK\":\n",
    "        location = data[\"results\"][0][\"geometry\"][\"location\"]\n",
    "        latitude = location[\"lat\"]\n",
    "        longitude = location[\"lng\"]\n",
    "        return latitude, longitude\n",
    "    else:\n",
    "        print(\"Error:\", data.get(\"status\", \"Unknown error\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    city = input(\"Enter the name of the city: \")\n",
    "    coordinates = get_coordinates(city)\n",
    "\n",
    "    if coordinates:\n",
    "        print(f\"Coordinates of {city}: Latitude = {coordinates[0]}, Longitude = {coordinates[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24dfb85",
   "metadata": {},
   "source": [
    "# 6. Write a program to scrap all the available details of best gaming laptops from digit.in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d316244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_laptop_details(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        laptop_list = []\n",
    "        \n",
    "        laptops = soup.find_all('div', class_='TopNumbeHeading sticky-footer')\n",
    "        for laptop in laptops:\n",
    "            laptop_details = {}\n",
    "            \n",
    "            name = laptop.find('div', class_='TopNumbeHeading sticky-footer').text.strip()\n",
    "            specs = laptop.find('div', class_='TopNumbeSecondHeading sticky-footer').text.strip()\n",
    "            \n",
    "            laptop_details['Name'] = name\n",
    "            laptop_details['Specifications'] = specs\n",
    "            \n",
    "            laptop_list.append(laptop_details)\n",
    "        \n",
    "        return laptop_list\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage.\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = 'https://www.digit.in/top-products/best-gaming-laptops-40.html'\n",
    "    laptop_details = get_laptop_details(url)\n",
    "    \n",
    "    for index, laptop in enumerate(laptop_details, start=1):\n",
    "        print(f\"Laptop {index}:\")\n",
    "        print(f\"Name: {laptop['Name']}\")\n",
    "        print(f\"Specifications: {laptop['Specifications']}\")\n",
    "        print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f68933",
   "metadata": {},
   "source": [
    "# 7. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped:\n",
    "“Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55cdf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_forbes_billionaires():\n",
    "    url = \"https://www.forbes.com/billionaires/\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        billionaires = []\n",
    "        rows = soup.select(\".data\")[0].find_all(\"tr\")\n",
    "        \n",
    "        for row in rows[1:]:  # Skipping the header row\n",
    "            columns = row.find_all(\"td\")\n",
    "            rank = columns[0].text.strip()\n",
    "            name = columns[1].text.strip()\n",
    "            net_worth = columns[2].text.strip()\n",
    "            age = columns[3].text.strip()\n",
    "            citizenship = columns[4].text.strip()\n",
    "            source = columns[5].text.strip()\n",
    "            industry = columns[6].text.strip()\n",
    "            \n",
    "            billionaire = {\n",
    "                \"Rank\": rank,\n",
    "                \"Name\": name,\n",
    "                \"Net worth\": net_worth,\n",
    "                \"Age\": age,\n",
    "                \"Citizenship\": citizenship,\n",
    "                \"Source\": source,\n",
    "                \"Industry\": industry\n",
    "            }\n",
    "            \n",
    "            billionaires.append(billionaire)\n",
    "        \n",
    "        return billionaires\n",
    "    else:\n",
    "        print(\"Failed to fetch data\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    billionaires_list = scrape_forbes_billionaires()\n",
    "    \n",
    "    for billionaire in billionaires_list:\n",
    "        print(\"Billionaire Details:\")\n",
    "        for key, value in billionaire.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        print(\"=\" * 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1613cd77",
   "metadata": {},
   "source": [
    "# 8. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted\n",
    "from any YouTube Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c7faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Set your API key here\n",
    "API_KEY = \"YOUR_YOUTUBE_API_KEY\"\n",
    "VIDEO_ID = \"YOUTUBE_VIDEO_ID\"\n",
    "\n",
    "# Create a YouTube Data API client\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "def get_video_comments(youtube, **kwargs):\n",
    "    comments = []\n",
    "    results = youtube.commentThreads().list(**kwargs).execute()\n",
    "    \n",
    "    while results:\n",
    "        for item in results['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comments.append({\n",
    "                'text': comment['textDisplay'],\n",
    "                'time': comment['publishedAt'],\n",
    "                'upvotes': comment['likeCount']\n",
    "            })\n",
    "\n",
    "        # Check if there are more comments\n",
    "        if 'nextPageToken' in results:\n",
    "            kwargs['pageToken'] = results['nextPageToken']\n",
    "            results = youtube.commentThreads().list(**kwargs).execute()\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return comments\n",
    "\n",
    "# Fetch comments from the video\n",
    "comments = get_video_comments(youtube, part='snippet', videoId=VIDEO_ID, textFormat='plainText')\n",
    "\n",
    "# Save the comments to a JSON file\n",
    "output_file = 'youtube_comments.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(comments, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f'Successfully extracted {len(comments)} comments and saved to {output_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e8525b",
   "metadata": {},
   "source": [
    "# 9. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in\n",
    "“London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall\n",
    "reviews, privates from price, dorms from price, facilities and property description. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680abe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = \"https://www.hostelworld.com/search?search_keywords=London,%20England&country=England&city=London&date_from=2023-09-01&date_to=2023-09-10\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all hostel containers\n",
    "hostel_containers = soup.find_all('div', class_='hostel-container')\n",
    "\n",
    "# Initialize a list to store hostel data\n",
    "hostel_data = []\n",
    "\n",
    "# Iterate through each hostel container\n",
    "for container in hostel_containers:\n",
    "    name = container.find('h2', class_='title').text.strip()\n",
    "    distance = container.find('span', class_='distance').text.strip()\n",
    "    rating = container.find('div', class_='score orange').text.strip()\n",
    "    total_reviews = container.find('div', class_='reviews').text.strip()\n",
    "    overall_reviews = container.find('div', class_='rating').text.strip()\n",
    "    privates_price = container.find('div', class_='price-col').find('a', class_='prices').text.strip()\n",
    "    dorms_price = container.find('div', class_='price-col').find('a', class_='prices', href=re.compile(r'dorms')).text.strip()\n",
    "    facilities = [item.text.strip() for item in container.find_all('span', class_='facilities-label')]\n",
    "    description = container.find('div', class_='ratings').find_next('p').text.strip()\n",
    "    \n",
    "    hostel_data.append({\n",
    "        'Name': name,\n",
    "        'Distance from City Centre': distance,\n",
    "        'Rating': rating,\n",
    "        'Total Reviews': total_reviews,\n",
    "        'Overall Reviews': overall_reviews,\n",
    "        'Privates from Price': privates_price,\n",
    "        'Dorms from Price': dorms_price,\n",
    "        'Facilities': facilities,\n",
    "        'Description': description\n",
    "    })\n",
    "\n",
    "# Print the scraped data\n",
    "for hostel in hostel_data:\n",
    "    print(hostel)\n",
    "    print('-' * 50)\n",
    "\n",
    "# You can also save the data to a file (e.g., CSV or JSON) if needed\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
